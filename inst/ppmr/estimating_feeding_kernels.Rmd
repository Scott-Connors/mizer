---
title: "Estimating feeding kernels from stomach data"
author: "Gustav Delius"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
```

## Introduction
We want to use available observations of the size distribution of prey items in
predator stomachs to estimate the feeding kernels to use in size-spectrum
models. 

We will initially use data for sardine and anchovy provided by Mariella and the
[Barnes
dataset](http://www.esajournals.org/doi/abs/10.1890/07-1551.1?journalCode=ecol).
Later we will want to extend that to the [DAPSTOM
dataset](https://www.cefas.co.uk/media/41463/dapstom-phase-4-report-2014-dlm.pdf)
and a [dataset of diets of North Atlantic
fishes](http://www.fishecology.org/diets/diets.htm)

In addition to the results, this document contains all the code to
reproduce our analysis.

## Density functions
In order to avoid confusion, I will discuss in detail the different ways in
which we could describe the size distribution of the stomach items in terms of
different density functions. You can skip this section because the concepts of
number density and biomass density are probably intuitively clear. You can come
back to here if at some point later you get confused.

### Probability densities
First of all we can describe the distribution in terms of the probability
distribution of various variables. We can use either the prey size, or the
predator/prey mass ratio, or the log of the predator/prey mass ratio.

Let us denote by $f_w(w|w')$ the probability density that a random prey item in
a stomach of a predator of size $w'$ has size $w$. The sizes of the actual
observed prey items are seen as samples from this probability distribution.

Rather than working with the prey size $w$, we can also work with the
predator/prey mass ratio $r = w' / w$. The distribution of this is then

$$
f_r(r|w') = \left|\frac{dw}{dr}\right| f_w(r|w') = \frac{w^2}{w'} f_w(w|w').
$$

It is actually a good idea to work with the logarithm of the predator/prey mass
ratio $l = log(r) = \log(w'/w)$. Its distribution is
$$
f_l(l|w') = \left|\frac{dw}{dl}\right| f_w(w|w') = w f_w(w|w').
$$

We will be making the fundamental assumption that the distribution of the
predator/prey mass ratio is independent of the predator size: 
$f_r(r|w') = f_r(r)$ (and hence also $f_l(l|w') = f_l(l)$). Of course we
should look at the data to see if this assumption is reasonable.

### Number densities
We can estimate these probability densities from the data by binning the data in
equally sized bins of the relevant variable and plotting histograms of the
number of prey items in each bin. To get the correct normalisation of the
densities the height of the bars should be equal to the number of prey items
divided by the width of the bin divided by the total number of prey items. That
way the area under the histogram will be equal to 1.

Let us introduce the number density $N_w(w|w')$ so that the number of prey items
with sizes between $w$ and $w+dw$ observed in stomachs of predators of size $w'$
is $N_w(w|w')dw$. Then in the limit of infinite sample size
$$
f_w(w|w') = \frac{N_w(w|w')}{\int N_w(\tilde{w}|w')d\tilde{w}}.
$$
The total number of individuals in stomachs of predators of size $w'$ depends
of course on how many stomachs were sampled and is not of interest to us, so
we do not loose any information by going to the normalised density $f_w(w|w')$.

Similarly we can introduce the number density $N_l(l|w')$ so that 
$N_l(l|w')dl$ is the number of prey items with log predator/prey size ratio
between $l$ and $l+dl$. Then
$$
f_l(w) = f_l(l|w') = \frac{N_l(l|w')}{\int N_l(\tilde{l}|w')d\tilde{l}}.
$$
If we have data from predators of different sizes we can combine them because
we assume that the distribution is independent of the predator size:
$$
f_l(w) = \frac{N_l(l)}{\int N_l(\tilde{l})d\tilde{l}}.
$$
This is the advantage of working in a distribution in predator/prey mass
ratio (or its log) instead of the prey size.

### Biomass densities
Instead of looking at the histogram where the height of the bars is proportional
to the number of individuals in a bin, we can also look at the histogram where
the height is proportional to the total biomass in the bin. So instead of
counting the number of individuals in each bin, we sum up their biomasses.

Or, working with the associated densities, we can look at the biomass density
$B_w(w|w')$ so that $B_w(w|w')dw$ is the total biomass of prey items with sizes
between $w$ and $w+dw$. Then
$$
B_w(w|w') = w\ N_w(w|w').
$$
Using bins of log predator/prey size ratio instead, we have the biomass
density $B_l(w|w')$ so that the total biomass in the bin from $l$ to $l+dl$
is $B_l(l|w')dl$:
$$
\begin{split}
B_l(l|w') &= \left|\frac{dw}{dl}\right|B_w(w|w') = w\,B_w(w|w') = w^2\,N_w(w|w')\\
&= w\,N_l(l|w') = w' e^{-l} N_l(l) .
\end{split}
$$
The binned biomass is obviously not independent of the predator size $w'$, but
again, because the overall observed biomass is dependent on sampling effort and
does not hold information for us, we are only interested in the normalised
biomass density. Here the $w'$ dependence cancels out:
$$
b_l(l|w') = \frac{B_l(l|w')}{\int B_l(l|w')dl} 
= \frac{e^{-l} N_l(l)}{\int e^{-\tilde{l}} N_l(\tilde{l})d\tilde{l}} = b_l(l).
$$
So again we can combine the observations from all predator sizes to get the best
estimate of the biomass distribution as a function of $l$.

Because $N_l(l)$ is proportional to the probability density $f_l(l)$ we also
have
$$
b_l(l) \propto e^{-l} f_l(l).
$$

## Mariella's dataset

Mariella has data of stomach content for anchovy and sardine. Mariella can
provide more details of the origin of this data.
```{r message=FALSE, warning=FALSE}
ppmr <- read_csv("inst/humboldt/Data_PPMR.csv")
ppmr
```

The `wprey` variable is the typical weight of an individual of a particular prey
species and `Nprey` is the average number of individuals of that species in a
stomach and `wpredator` is the weight of the predator. The table reports the
same predator size for all anchovy and the same predator size for all sardine
because all predators of each species had very similar sizes.

We capitalise the species names and add a column for the log of the
predator/prey mass ratio.
```{r}
ppmr <- ppmr %>% 
    mutate(Species = str_to_title(Species),
         logpredprey = log(wpredator / wprey))
ppmr
```

Let's look at the smallest and largest prey items as well as the log of the
smallest and largest predator/prey mass ratios in the data:
```{r}
ppmr %>% 
    group_by(Species) %>% 
    summarise(min_wprey = min(wprey),
              max_wprey = max(wprey),
              min_logpredprey = min(logpredprey),
              max_logpredprey = max(logpredprey))
```
We can not say for sure that anchovy and sardine do not eat larger prey than
contained in the dataset, because large prey are rare and even the observed
large prey only hav a very small count:
```{r}
ppmr %>% 
    group_by(Species) %>% 
    filter(wprey == max(wprey))
```
Remember that `Nprey` is the average number of prey individuals of that size in
a stomach. So the largest prey item observed in an anchovy stomach only showed
up in one of 10.000 stomachs. One would need a very large sample of stomachs to
have a good chance to observe any larger prey. So to get a better value for the
largest possible prey one should look at the morphology of the predator species.

Similarly, we can not be certain of the true size of the smallest possible prey
items because it may just have been to difficult to identify cells of the
smallest plankton prey, especially as they will be digested very quickly. Again
it may make sense to complement this study by a look at the morphology of the
gill rakers, although one has to acknowledge that gill rakers can, when they
become clogged, filter out smaller individuals than would be indicated by the
spacing of the rakers.

We will have more discussion of the relation between the stomach data and the
actual diet of the predator later. For now we concentrate on investigating the
stomach data.

There are two ways to visualise the size-distribution of the prey items: we can
bin the data and plot histograms, or we can plot density kernel estimates. We'll
start with a look at binned data.

### Binned data

We use size bins that are equally-spaced on a logarithmic axis between the
smallest and largest predator/prey size ratio. For later use we create a vector
`breaks` with the bin boundaries:
```{r}
no_bins <- 30  # Number of bins
binsize <- (max(ppmr$logpredprey) - min(ppmr$logpredprey)) / (no_bins - 1)
breaks <- seq(min(ppmr$logpredprey) - binsize/2,
              by = binsize, length.out = no_bins + 1)
```
We now add up both the numbers and the biomass in each bin and then normalise so
that the area under each histogram is 1. We also add a column `l` with the log
of predator/prey mass ratio at the centre of the each bin.
```{r}
binned_ppmr <- ppmr %>% 
    # bin data
    mutate(cut = cut(logpredprey, breaks = breaks, right = FALSE,
                     labels = FALSE)) %>% 
    group_by(Species, cut) %>% 
    summarise(Numbers = sum(Nprey), 
              Biomass = sum(Nprey * wprey)) %>% 
    # normalise
    mutate(Numbers = Numbers / sum(Numbers) / binsize,
           Biomass = Biomass / sum(Biomass) / binsize)  %>%
    # column for predator/prey size ratio
    mutate(l = map_dbl(cut, function(idx) breaks[idx] + binsize/2))
binned_ppmr
```
We convert this into the long table format preferred by ggplot2.
```{r}
binned_ppmr <- binned_ppmr %>%
gather(key = "Type", value = "Density", Numbers, Biomass)
```
We can now easily plot the histograms.
```{r}
binned_ppmr %>% 
  ggplot(aes(l, Density, fill = Type)) +
    geom_col(position = "dodge") +
    facet_grid(rows = vars(Species), scales = "free_y") +
    xlab("Log of Predator/Prey mass ratio") +
    expand_limits(x = c(0, 30))
```
Expressed in terms of the notation from the section on density functions, these
histograms are estimates of the normalised number distribution $f_l(l)$ and the
normalised biomass distribution $b_l(l)$.

Note how the biomass of prey is concentrated at very different predator/prey
mass ratio than the number of prey. The information about the biomass
distribution is contained mostly in the left tail of the number distribution and
vice versa the information about the number distribution is mostly contained in
the right tail of the biomass distribution.

### Density estimates
An alternative way to estimate densities from data is provided by the kernel
density estimation method. Here rather than aggregating observations in bins,
each observation contributes its own little Gaussian centred at that data value.
The estimated density function is obtained by summing all these little 
Gaussians together. That produces a smoother estimate than the histogram method.
The width of the individual Gaussians plays the same role as the bins size.
There are some heuristics that allow R to choose a sensible width that provides
a good summary of the data.
```{r}
adjust <- 1/2  # decrease bandwidth for kernel estimate
ppmr <- ppmr %>% 
    group_by(Species) %>% 
    mutate(weight_numbers = Nprey / sum(Nprey),
           weight_biomass = Nprey * wprey / sum(Nprey * wprey))
ggplot(ppmr) +
    geom_density(aes(logpredprey, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(logpredprey, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    facet_grid(rows = vars(Species), scales = "free_y") +
    xlab("Log of predator/prey mass ratio") +
    expand_limits(x = c(0, 29))
```


### Gaussian does not fit
Let us start by observing that it would be very inappropriate to use a Gaussian
curve to describe these distributions. We illustrate this in the case of
the Anchovy, where the number distribution looks at least somewhat similar to a
normal distribution.

We plot the normal distribution with the same mean and standard deviation as the
data on top of the histogram for the number distribution.
```{r}
chosen_species = "Anchovy"
weighted.sd <- function(x, w) {
  sqrt(sum(w * (x - weighted.mean(x, w))^2))
}
fit <- ppmr %>% 
  filter(Species == chosen_species) %>% 
  summarise(mean = weighted.mean(logpredprey, weight_numbers),
            sd = weighted.sd(logpredprey, weight_numbers))
ppmr %>% 
  filter(Species == chosen_species) %>% 
  ggplot() +
    geom_density(aes(logpredprey, weight = weight_numbers),
                 fill = "#00BFC4", adjust = adjust) +
    xlab("Log of Predator/Prey mass ratio") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean, 
                              sd = fit$sd), 
                  colour = "blue") +
    expand_limits(x = c(6, 30))
```
This may not look so bad. However,
if the size distribution was correctly described by the normal distribution,
i.e., if
$$
f_l(l) \propto \exp\left(-\frac{(l-\log(\beta))^2}
  {2\sigma^2}\right),
$$
then the biomass density would be given by
$$
\begin{split}
b_l(l) &= e^{-l}f_l(l)
\propto \exp\left(-\frac{(l-\log(\beta))^2}
  {2\sigma^2} - l\right)\\
&\propto \exp\left(-\frac{(l-(\log(\beta)-\sigma^2)^2}
  {2\sigma^2}\right)
\end{split}
$$
Thus the biomass would also be normally distributed with the mean
shifted by $\sigma^2$. Putting this into the same picture gives
```{r}
ppmr %>% 
  filter(Species == chosen_species) %>% 
  ggplot() +
    geom_density(aes(logpredprey, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(logpredprey, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    xlab("Log of Predator/Prey mass ratio") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean, 
                              sd = fit$sd), 
                  colour = "blue") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean - fit$sd^2, 
                              sd = fit$sd), 
                  colour = "red") +
    expand_limits(x = c(0, 30))
```
This does not fit the actual biomass distribution at all! 

For the Sardine the normal distribution fits neither the number density
nor the biomass density.
```{r}
chosen_species = "Sardine"
weighted.sd <- function(x, w) {
  sqrt(sum(w * (x - weighted.mean(x, w))^2))
}
fit <- ppmr %>% 
  filter(Species == chosen_species) %>% 
  summarise(mean = weighted.mean(logpredprey, weight_numbers),
            sd = weighted.sd(logpredprey, weight_numbers))
ppmr %>% 
  filter(Species == chosen_species) %>% 
  ggplot() +
    geom_density(aes(logpredprey, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(logpredprey, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    xlab("Log of Predator/Prey mass ratio") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean, 
                              sd = fit$sd), 
                  colour = "blue") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean - fit$sd^2, 
                              sd = fit$sd), 
                  colour = "red") +
    expand_limits(x = c(0, 30))
```


### Exponential fits better
We have learned above that the small number of large individuals in the
stomach provide the dominant contribution to the biomass consumed by
the predator and hence are important. To make them more visible, we show
the histograms again but now with a logarithmic y-axis
```{r}
binned_ppmr %>% 
  ggplot(aes(l, Density)) +
    geom_col() +
    facet_grid(Species ~ Type, scales = "free_y") +
    xlab("Log of Predator/Prey mass ratio") +
    scale_y_continuous(trans = "log")
```
This motivates us to try to fit a truncated exponential distribution to the
data (which would be a truncated power-law distribution when working with the
predator/prey mass ratio instead of its logarithm).

```{r}
binned_ppmr %>% 
  ggplot(aes(l, Density)) +
    geom_col() +
    geom_smooth(method = "lm", se = FALSE) +
    facet_grid(Species ~ Type, scales = "free_y") +
    xlab("Log of Predator/Prey mass ratio") +
    scale_y_continuous(trans = "log")
```
Of course again we have the problem that the biomass density is not independent
of the number density, so fitting separate distributions to the two is not
appropriate. If we assume that the number distribution follows the exponential
distribution, i.e.,
$$
f_l(l)\propto \exp(\alpha\ l)
$$
for some parameter $\alpha$ then the biomass distribution follows an
exponential distribution with parameter $\alpha - 1$:
$$
b_l(l) = e^{-l}f_l(l)
\propto \exp((\alpha - 1)l).
$$
Amazingly, the parameters of the independently fitted distributions do almost
perfectly have this relationship:
```{r}
binned_ppmr %>% 
  group_by(Species, Type) %>% 
  group_modify(~ broom::tidy(lm(log10(Density) ~ l, data = .x))) %>% 
  filter(term == "l")
```
The parameter of the biomass distribution is almost exactly 1 less than the parameter of the numbers distribution.

### Maximum likelihood
Fitting the exponential distribution by fitting a linear model to the logged
histogram is not really appropriate. A better method might be to fit directly to
the data using the maximum likelihood principle.

If we assume that we know the upper and lower cutoff of the distribution and
only need to estimate the exponent, then that is very easy.
The density of the truncated exponential distribution is
$$
f_l(l) = \begin{cases}
\frac{1}{Z}\exp(\alpha\ l) &\text{ if }l\in[l_{min}, l_{max}]\\
0 &\text{ otherwise,}
\end{cases}
$$
where
$$
Z = \int_{l_{min}}^{l_{max}}\exp(\alpha\ l)dl=
\frac{1}{\alpha}\left(\exp(\alpha\ l_{max})-\exp(\alpha\ l_{min})\right).
$$
Thus the log likelihood function is
$$
\begin{split}
l(\alpha) &= \sum_i\left(\alpha\ l_i 
- \log\left(\frac{1}{\alpha}\left(\exp(\alpha\ l_{max})-\exp(\alpha\ l_{min})\right)\right)\right)\\
&=\alpha \sum_il_i + n\log(\alpha) 
- n\log\left(\exp(\alpha\ l_{max})-\exp(\alpha\ l_{min})\right)
\end{split}
$$
In order to avoid having to minimize this numerically, we now make the 
approximation that $\alpha$ is large enough so that $\exp(\alpha\ l_{min})$ is 
negligibly small compared to $\exp(\alpha\ l_{max})$. Then
$$
l(\alpha)\approx\alpha \sum_il_i + n\log(\alpha) 
- n\ \alpha\ l_{max}
$$
To find the value of $\alpha$ that maximizes this likelihood we set
$$
0=\frac{dl(\alpha)}{d\alpha} \approx \sum_il_i+\frac{n}{\alpha}-n\ l_{max}
$$
and solve for $\alpha$. This gives
$$
\alpha \approx \frac{1}{l_{max}-\bar{l}}
$$
where $\bar{l}$ is the average over all $l_i$.

```{r}
summary <- ppmr %>% 
  group_by(Species) %>% 
  summarise(lbar = weighted.mean(logpredprey, weight_numbers),
            lmax = max(logpredprey),
            lmin = min(logpredprey),
            alpha = 1/(lmax - lbar))
summary
```
Note how sensitive these estimates are to the value of `lmax`. To see why these are not good estimates, let us have a look what the fit looks like.
```{r}
dnumbers <- function(l, alpha, lmax) {
  d <- as.numeric(l <= lmax)
  d[d > 0] <- dexp(lmax - l[d > 0], alpha)
  return(d)
}
dbiomass <- function(l, alpha, lmin) {
  d <- as.numeric(l >= lmin)
  d[d > 0] <- dexp(l[d > 0] - lmin, 1 - alpha)
  return(d)
}
selected_species <- "Sardine"
ppmr %>% 
  filter(Species == selected_species) %>% 
  ggplot() +
    geom_density(aes(logpredprey, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(logpredprey, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    stat_function(fun = dnumbers, 
                  args = list(alpha = summary$alpha[summary$Species == selected_species], 
                              lmax = summary$lmax[summary$Species == selected_species]), 
                  colour = "blue") +
    stat_function(fun = dbiomass, 
                  args = list(alpha = summary$alpha[summary$Species == selected_species], 
                              lmin = summary$lmin[summary$Species == selected_species]), 
                  colour = "red") +
    xlab("Log of Predator/Prey mass ratio")  +
    expand_limits(x = c(0, 30))
```
Clearly the density should not rise exponentiall all the way to the largest
observed value `lmax` but there should be a smoother cutoff. There should also
be a smoother cutoff at `lmin`.

### Sigmoidal cutoffs
The simplest way to implement smoother cutoffs at the end of the distributions
is to multiply by two sigmoidal functions: a rising one centred near `lmin`
and a falling one near `lmax`. So we choose
$$
f_l(l) \propto \frac{\exp(\alpha\ l)}
{\left(1+e^{u_l(l_l - l)}\right)
\left(1+e^{u_r(l_r-l)}\right)}.
$$
We have introduced 4 new parameters into the distribution: $l_l$ is the log of
the predator/prey mass ratio around which the sigmoidal switch-on happens and
$u_l$ determines the steepness of that switch-on. Similarly $l_r$ is the log of
the predator/prey mass ratio around which the sigmoidal switch-off happens and
$u_r$ determines the steepness of that switch-off.

The probability density $f_r(r)$ of the predator/prey mass ratio $r=e^l$ is
then a power-law distribution with exponent $\alpha - 1$ and sigmoidal cut-offs:
$$
f_r(r) \propto e^{-l}f_l(l)
= \frac{r^{\alpha - 1}}
{\left(1+\left(\frac{e^{l_l}}{r}\right)^{u_l}\right)
\left(1+\left(\frac{r}{e^{l_r}}\right)^{u_r}\right)}.
$$

It would require numerical methods to do a maximum likelihood estimation of
these parameters from the data, and it is not even clear that such a maximum
likelihood estimation, that weighs each observation equally, would be
appropriate, given that during observation, small individuals will be easier to
miss than large ones and thus there would be more uncertainty in the
observations at large predator/prey mass ratio. So instead, we will simply do a
visual fit.
```{r}
source("inst/humboldt/fit_kernel.R")
stomach <- filter(ppmr, Species == "Sardine")
pl <- fitKernel(stomach)
```



## Feeding kernel
Now we will discuss how the stomach content is related to the feeding kernel.

We describe the size distribution of prey in stomachs of predators of size $w'$
by the density function $N_w(w|w')$. This will be the result of a balance
between the rate at which prey items are ingested and the rate at which they are
digested. 

### Ingestion
The rate of ingestion $I(w|w')$ is proportional to the product of the feeding
kernel $\phi(w'/w)$ and the prey number density $N_c(w)$:
$$
I(w|w') \propto \phi(w'/w)\ N_c(w)
$$
The proportionality constant only depends on the predator size $w'$ and as we
will see, we will not need it in our analysis. The prey number density $N_c(w)$
is defined as usual such that $N_c(w)dw$ is the total number of potential prey
items with sizes between $w$ and $w+dw$.

In a multi-species model in which the predator has different interaction
strengths with different prey species, given by an interaction matrix
$\theta_{ij}$, the prey number density for a predator of species $i$ would be
obtained from the number densities $N_j(w)$ of individual prey species as
$$
N_c(w) = \sum_j \theta_{ij}N_j(w).
$$
Of course, we do not know the prey density that the predators experienced that
were sampled in the stomach observation. So the best we can do is to assume that
it was reasonably close to the Sheldon spectrum:
$$
N_c(w)\propto w^{-\lambda}
$$
with $\lambda \approx 2$.

### Digestion

As prey items are digested, they loose biomass. Thus the observed size of a prey
item will not be the original size at which it was ingested. Let us denote by
$D(w)$ the rate at which a prey item of size $w$ looses weight due to digestion.

Of course we do not know in detail the rate at which prey items are digested.
However it is reasonable to make the approximation that the rate scales with
body size as $w^{2/3}$ because digestion acts on the surface of the prey item
and this surface scales approximately as $w^{2/3}$. Thus we will assume that
$D(w|w')\propto w^{2/3}$, with the proportionality constant depending only on
the predator. 
<!-- Because, as mentioned before, predator-dependent constants will be irrelevant in -->
<!-- our discussion, we will also drop the $|w'$ in our equations from now on. -->

It may also be the case that prey items become unidentifiable once a certain
fixed proportion of their body mass is digested away. So rather than being
recorded as a prey of smaller size, they would not be recorded at all. The time
until this proportion of a prey item is digested away is proportional to the
body mass divided by the rate at which mass is digested: 
$T\propto w/D(w|w') \propto w^{1/3}$. Thus the rate at which individuals become
unidentifiable is $U(w|w')\propto w^{-1/3}$.

The density of observed prey items at a particular size will increase when such
a prey item is ingested or if a larger prey item is digested down to this size,
and the density will decrease when a prey item is digested down to a smaller
size or becomes unidentifiable. This leads to the partial differential equation
$$
\frac{N_w(w|w')}{dt} = I(w|w') +\partial_w\left(D(w|w')N_w(w|w')\right)
- U(w|w')N_w(w|w').
$$
We assume that on average the stomach observations reflect the steady state
with $dN/dt=0$. We now substitute our expressions for the various rates into
the resulting balance equation:
$$
0 = i(w') \phi(w'/w)\ w^{-\lambda} 
+ \frac{\partial}{\partial_w}\left(d(w')w^{2/3}N_w(w|w')\right)
- u(w')w^{-1/3}N_w(w|w')
$$


#### Items become unidentifiable
In the first model, we assume that digestion results in a finite time during
which the prey item stays identifiable. After that time, the prey item will
be considered as lost to the stomach analysis process. It will be too decayed
to be counted. Let us assume that this time is proportional to $w^\alpha$ for
some exponent $\alpha$. 


the rate at which mass is digested is proportional
to $w^{2/3}. Let us assume that a prey item becomes unidentifiable when
a fixed percentage of its body mass is digested. The time until that has 
happened is proportional to the body mass divided by the rate at which mass
is digested: $T\propto w/w^{2/3} = w^{1/3}$. The rate at which prey items of
size $w$


### Predation rate



## Barnes dataset

## Summary